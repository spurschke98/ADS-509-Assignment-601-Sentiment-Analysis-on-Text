{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Sentiment Assignment\n",
    "\n",
    "This notebook holds the Sentiment Assignment for Module 6 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In a previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we apply sentiment analysis to those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emojis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import copy\n",
    "import matplotlib as plt\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from string import punctuation\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/summerpurschke/Desktop/ADS/ADS509/Mod2\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"/twitter/\"\n",
    "lyrics_folder = \"/lyrics\"\n",
    "\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "tidy_text_file = \"tidytext_sentiments.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A Pandas data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "def create_lyrics_dictionary(folder_path):\n",
    "    lyrics_dict = {}\n",
    "\n",
    "    # Iterate over each item in the folder\n",
    "    for item_name in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item_name)\n",
    "\n",
    "        # Check if the item is a folder (artist folder)\n",
    "        if os.path.isdir(item_path):\n",
    "            artist_dict = {}\n",
    "\n",
    "            # Iterate over each file in the artist folder\n",
    "            for filename in os.listdir(item_path):\n",
    "                file_path = os.path.join(item_path, filename)\n",
    "\n",
    "                # Check if the item is a file (song file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path, \"r\") as file:\n",
    "                        # Read the contents of the file\n",
    "                        lines = file.readlines()\n",
    "\n",
    "                        if lines:\n",
    "                            title = lines[0].strip()  # Assuming the first line contains the title\n",
    "                            lyrics = ''.join(lines[1:])  # Combine the remaining lines as the lyrics\n",
    "\n",
    "                            # Add the song lyrics to the artist's dictionary with the title as the inner key\n",
    "                            artist_dict[title] = lyrics\n",
    "\n",
    "            # Add the artist's dictionary to the main lyrics dictionary\n",
    "            lyrics_dict[item_name] = artist_dict\n",
    "\n",
    "    return lyrics_dict\n",
    "\n",
    "# Call the function with the lyrics folder path\n",
    "lyrics_dictionary = create_lyrics_dictionary(data_location + lyrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "\n",
    "twitter_files = os.listdir(data_location + twitter_folder)\n",
    "desc_files = [f for f in twitter_files if \"followers_data\" in f]\n",
    "twitter_data = defaultdict(list)\n",
    "for f in desc_files :\n",
    "    artist = f.split(\"_\")[0]\n",
    "        \n",
    "    with open(data_location + twitter_folder + f,'r', encoding='utf-8') as infile :\n",
    "        next(infile)\n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if len(line) == 7 :\n",
    "                twitter_data[artist].append(line[6])\n",
    "\n",
    "twitter_data = dict(twitter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the positive and negative words and the\n",
    "# tidytext sentiment. Store these so that the positive\n",
    "# words are associated with a score of +1 and negative words\n",
    "# are associated with a score of -1. You can use a dataframe or a \n",
    "# dictionary for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e171f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in positive words\n",
    "positive_words = pd.read_csv(\"positive-words.txt\", header=None, comment=\";\", encoding='latin-1')\n",
    "positive_words.columns = [\"word\"]\n",
    "positive_words[\"sentiment\"] = 1\n",
    "\n",
    "# Read in negative words\n",
    "negative_words = pd.read_csv(\"negative-words.txt\", header=None, comment=\";\", encoding='latin-1')\n",
    "negative_words.columns = [\"word\"]\n",
    "negative_words[\"sentiment\"] = -1\n",
    "\n",
    "# Read in tidytext sentiment\n",
    "tidytext_sentiment = pd.read_csv(\"tidytext_sentiments.txt\", sep=\"\\t\")\n",
    "tidytext_sentiment = tidytext_sentiment.replace('negative', -1)\n",
    "tidytext_sentiment = tidytext_sentiment.replace('positive', 1)\n",
    "\n",
    "# # Combine positive, negative, and tidytext sentiment into a single dataframe or dictionary\n",
    "sentiment_scores = pd.concat([positive_words, negative_words, tidytext_sentiment])\n",
    "sentiment_scores.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop duplicate values \n",
    "sentiment_scores = sentiment_scores.drop(columns = 'lexicon').drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Songs\n",
    "\n",
    "In this section, score the sentiment for all the songs for both artists in your data set. Score the sentiment by manually calculating the sentiment using the combined lexicons provided in this repository. \n",
    "\n",
    "After you have calculated these sentiments, answer the questions at the end of this section.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8420e608",
   "metadata": {},
   "source": [
    "Clean the lyrics data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664f8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison\n",
    "\n",
    "# Define function for removing punctuation from dictionary \n",
    "def clean_text(text):\n",
    "    cleaned_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return cleaned_text\n",
    "\n",
    "# Initiate a new dictionary that is empty - will add cleaned data as it's processed \n",
    "lyrics_dictionary_cleaned = {}\n",
    "\n",
    "# Fold to lowercase and populate lyrics_dictionary_cleaned\n",
    "for artist, songs in lyrics_dictionary.items():\n",
    "    cleaned_songs = {}\n",
    "    for song, lyrics in songs.items():\n",
    "        cleaned_lyrics = lyrics.lower()\n",
    "        cleaned_songs[song] = cleaned_lyrics\n",
    "    lyrics_dictionary_cleaned[artist] = cleaned_songs\n",
    "\n",
    "# Remove stopwords directly from lyrics_dictionary_cleaned\n",
    "for artist, songs in lyrics_dictionary_cleaned.items():\n",
    "    for song, lyrics in songs.items():\n",
    "        # Split the lyrics into individual words\n",
    "        words = lyrics.split()\n",
    "        # Remove stopwords from the list of words\n",
    "        cleaned_words = [word for word in words if word.lower() not in sw]\n",
    "        # Join the cleaned words back into a single string\n",
    "        cleaned_lyrics = \" \".join(cleaned_words)\n",
    "        # Update the lyrics in the lyrics_dictionary_cleaned\n",
    "        lyrics_dictionary_cleaned[artist][song] = cleaned_lyrics\n",
    "\n",
    "# Remove punctuation directly from lyrics_dictionary_cleaned\n",
    "for artist, songs in lyrics_dictionary_cleaned.items():\n",
    "    cleaned_songs = {}\n",
    "    for song, lyrics in songs.items():\n",
    "        # Remove punctuation marks\n",
    "        cleaned_lyrics = lyrics.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        cleaned_songs[song] = cleaned_lyrics\n",
    "    lyrics_dictionary_cleaned[artist] = cleaned_songs\n",
    "\n",
    "# split at whitespace \n",
    "for artist, songs in lyrics_dictionary_cleaned.items():\n",
    "    for song, lyrics in songs.items():\n",
    "        split_lyrics = lyrics.split()\n",
    "        lyrics_dictionary_cleaned[artist][song] = split_lyrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c1219eb",
   "metadata": {},
   "source": [
    "Calculate the sentiment scores for each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b06cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize an empty dictionary \n",
    "sentiment_dict = {}\n",
    "\n",
    "# Iterate over the artists in the lyrics_dictionary_cleaned\n",
    "for artist, songs in lyrics_dictionary_cleaned.items():\n",
    "    # Create a nested dictionary for the artist if it doesn't exist in the result dictionary\n",
    "    if artist not in sentiment_dict:\n",
    "        sentiment_dict[artist] = {}\n",
    "    \n",
    "    # Iterate over the songs for the artist\n",
    "    for song, tokens in songs.items():\n",
    "        # Initialize the sum for the current song - will add from here \n",
    "        song_sum = 0\n",
    "        \n",
    "        # Iterate over the tokens in the song\n",
    "        for token in tokens:\n",
    "            # Check if the token exists in the sentiment_scores dataframe\n",
    "            if token in sentiment_scores['word'].values:\n",
    "                # Get the sentiment value associated with the token\n",
    "                sentiment_value = sentiment_scores[sentiment_scores['word'] == token]['sentiment'].values[0]\n",
    "                # Add the sentiment value to the song_sum\n",
    "                song_sum += sentiment_value\n",
    "        \n",
    "        # Assign the song_sum to the result dictionary\n",
    "        sentiment_dict[artist][song] = song_sum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad5cf116",
   "metadata": {},
   "source": [
    "Create a dataframe of values for each song "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "519bdb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the flattened data\n",
    "flattened_data = []\n",
    "\n",
    "# Iterate over the outer keys (artists) in the result dictionary\n",
    "for artist, songs in sentiment_dict.items():\n",
    "    # Iterate over the inner keys (songs) in each artist's songs\n",
    "    for song, value in songs.items():\n",
    "        # Append the artist, song, and value as a tuple to the flattened_data list\n",
    "        flattened_data.append((artist, song, value))\n",
    "\n",
    "# Create a dataframe from the flattened_data list\n",
    "sentiment_df = pd.DataFrame.from_records(flattened_data, columns=['Artist', 'Song', 'Value'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b72f42c",
   "metadata": {},
   "source": [
    "### Questions\n",
    "Q: Overall, which artist has the higher average sentiment per song? \n",
    "\n",
    "A: Robyn has a higher average sentiment per song at 8.96 compared to 5.57. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cher songs have an average sentient of 5.57\n",
      "Robyn songs have an average sentient of 8.96\n"
     ]
    }
   ],
   "source": [
    "robyn_avg_sentiment = sentiment_df[sentiment_df['Artist'] == 'robyn']['Value'].mean()\n",
    "cher_avg_sentiment = sentiment_df[sentiment_df['Artist'] == 'cher']['Value'].mean()\n",
    "\n",
    "print(f'Cher songs have an average sentient of {round(cher_avg_sentiment,2)}')\n",
    "print(f'Robyn songs have an average sentient of {round(robyn_avg_sentiment,2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c9e5ca3",
   "metadata": {},
   "source": [
    "Q: For your first artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: \n",
    "\n",
    "Lowest Sentiments (in order):\n",
    "\n",
    "- Don't Fucking Tell me What To Do: This song is says killing in just about every line, which is probably driving the score.\n",
    "- Criminal Intent: This song is about partaking illegal activities (intentionally). \n",
    "- Love Kills: This song isn't as negative as the others in my opinion when reading through it, it's a song about the hardships of love which a lot of other artists have written about. However from a sentiment point of view, it has the word 'kill' often, though it is preceded by 'love' most of the times that I can see which likely balances out. It does however use words like protect, conceal, wreck, and cold which likely contribute to the negative score. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the three lowest sentiment songs for robyn \n",
    "robyn_lowest_3  = (sentiment_df[sentiment_df['Artist'] == 'robyn']).sort_values(by = 'Value').head(3)['Song'].to_list()\n",
    "\n",
    "# Change the value of i from 0 to 2 to print the lyrics of each of the three songs with the lowest sentiment value \n",
    "i = 2\n",
    "print(f'\\033[1m{robyn_lowest_3[i]}\\033[0m')  # Print the title in bold\n",
    "print(lyrics_dictionary['robyn'][robyn_lowest_3[i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22caf57f",
   "metadata": {},
   "source": [
    "Highest Sentiments (in order):\n",
    "- Love is Free: This song seems to have a lot of 'love' and 'free' tokens repeating, which are both positive words. Also, 'baby' is a positive word that is recurring \n",
    "- We Dance to the Beat: Most of the lyrics in this song are just the title, of which only 'dance' has a value associated with it which is positive. \n",
    "- Between the Lines: This song at first glance doesn't seem as positive as the last two, but the word baby occurs often which as we already know is positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872190e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the three highest sentiment songs for robyn \n",
    "robyn_highest_3  = (sentiment_df[sentiment_df['Artist'] == 'robyn']).sort_values(by = 'Value', ascending= False).head(3)['Song'].to_list()\n",
    "\n",
    "i = 2\n",
    "print(f'\\033[1m{robyn_highest_3[i]}\\033[0m')  # Print the title in bold\n",
    "print(lyrics_dictionary['robyn'][robyn_highest_3[i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9295b78e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f8334f4",
   "metadata": {},
   "source": [
    "Q: For your second artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: \n",
    "\n",
    "Lowest Sentiments (in order):\n",
    "- Bang-Bang: This song has songs like 'hit' 'down' and 'ground' in the chorus (which repeats) which are all negative. \n",
    "- Bang Bang (My Baby Shot Me Down): This song is very similar to the previous with slight changes, but is likely so negative for the same reasons. \n",
    "- Outrageous: This song repeats the word 'rage' often, and uses words like kill and lies. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36cf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\"Outrageous\"\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Outrageous, outrageous\n",
      "(They say) I'm outrageous\n",
      "It's the rage\n",
      "\n",
      "I'm gonna wear what I will and spend some\n",
      "And I will be dress to kill don'tcha know\n",
      "And when the lights come up\n",
      "I'm ready I'm ready\n",
      "To put on a show with class\n",
      "And if I clash it's cause I want to\n",
      "What a show and I want everyone to know\n",
      "They're gonna fly up, get an eyeful\n",
      "Everything that's craved from me\n",
      "I'm gonna be, I'm gonna be outrageous\n",
      "\n",
      "Outrageous\n",
      "(They say) I'm outrageous\n",
      "It's the rage it's the rage\n",
      "\n",
      "With my long black hair hanging way down to my\n",
      "Ask me no questions and I'll tell you no lies\n",
      "Don't tell me what to do don't tell me what to be\n",
      "See I don't trust anybody else's traits about make-up and me\n",
      "\n",
      "Well in my show I let everything go\n",
      "Is what you want is whatcha wanna see from me\n",
      "But when the curtain comes down\n",
      "And you're on your way back home\n",
      "I change into my jeans that are split at the seam\n",
      "I grab my funky black jacket and make quite a racket\n",
      "You drive like you're an outlaw\n",
      "Cause everything that's craved from me\n",
      "I'm gonna be, I'm gonna be outrageous\n",
      "\n",
      "So outrageous\n",
      "I'm outrageous honey yes a rage\n",
      "It's the rage\n",
      "Outrageous, outrageous\n",
      "I'm outrageous\n",
      "It's the rage it's a rage\n",
      "Outrageous, outrageous\n",
      "They say I'm outrageous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the three lowest sentiment songs for robyn \n",
    "cher_lowest_3  = (sentiment_df[sentiment_df['Artist'] == 'cher']).sort_values(by = 'Value').head(3)['Song'].to_list()\n",
    "\n",
    "# Change the value of i from 0 to 2 to print the lyrics of each of the three songs with the lowest sentiment value \n",
    "i = 2\n",
    "print(f'\\033[1m{cher_lowest_3[i]}\\033[0m')  # Print the title in bold\n",
    "print(lyrics_dictionary['cher'][cher_lowest_3[i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1a02464",
   "metadata": {},
   "source": [
    "Highest Sentiments (in order):\n",
    "- Love and Understanding: Just as the title suggests, this is an uplifting song about love and understanding and wanting more of both of them in the word - pretty positive if you ask me! \n",
    "- Perfection: This song right away doesn't look as positive to me as the previous, but it does have many positive words such as perfection, baby, best, and winner. \n",
    "- I Found You Love: Once again, cher over uses the words baby and love, leading to a positive sentiment of this song. This in my opinion is more positive than Love and Understanding and Perfection, based on how the sentences are constructed. However this sentiment calculation only considers the presence of the words not the combination of them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\"I Found You Love\"\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Well I was looking for a new love, a different kind of true love\n",
      "Who's gonna treat me right, all day and night\n",
      "Hey baby I've been looking too\n",
      "And I have found there's\n",
      "No other love from me but you\n",
      "Well I was looking for a new love\n",
      "A different kind of true love\n",
      "Who's gonna treat me right\n",
      "Day and night\n",
      "Well I found what I was after\n",
      "Now my life is filled with laughter\n",
      "I found you love\n",
      "I was lost with no direction\n",
      "Then my life was one big question\n",
      "I was down and out\n",
      "Filled with doubt\n",
      "Found what I was after\n",
      "Now my life is filled with laughter\n",
      "I found you love\n",
      "I found you love\n",
      "I found a new love\n",
      "He's wonderful and true\n",
      "He's gonna spent his money\n",
      "He's gonna call me honey\n",
      "I gonna tease her\n",
      "Oh Lord, I gonna squeeze her\n",
      "Gonna love her plenty\n",
      "She's gonna make me manly\n",
      "We're gonna hold love while we can\n",
      "I've been looking for a new love\n",
      "A different kind of true love\n",
      "Who's gonna treat me right\n",
      "Every day and every night\n",
      "Now I found what I was after\n",
      "Now my life is filled with laughter\n",
      "I found you love\n",
      "I found you love\n",
      "I found a new love\n",
      "He's wonderful and true\n",
      "He's gonna spent his money\n",
      "He's gonna call me honey\n",
      "I gonna please her\n",
      "Oh Lord, I gonna squeeze her\n",
      "Gonna love me madly\n",
      "I'm gonna love her gladly\n",
      "We're gonna hold love while we can\n",
      "We're gonna hold love while we can\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the three highest sentiment songs for robyn \n",
    "cher_highest_3  = (sentiment_df[sentiment_df['Artist'] == 'cher']).sort_values(by = 'Value', ascending= False).head(3)['Song'].to_list()\n",
    "\n",
    "i = 2\n",
    "print(f'\\033[1m{cher_highest_3[i]}\\033[0m')  # Print the title in bold\n",
    "print(lyrics_dictionary['cher'][cher_highest_3[i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "213ef6b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6cf9241",
   "metadata": {},
   "source": [
    "Q: Plot the distributions of the sentiment scores for both artists. You can use `seaborn` to plot densities or plot histograms in matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d53f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "\n",
    "# Plot histograms using matplotlib\n",
    "ax.hist(sentiment_df.loc[sentiment_df['Artist'] == 'robyn', 'Value'].values, alpha=0.5, label='robyn')\n",
    "ax.hist(sentiment_df.loc[sentiment_df['Artist'] == 'cher', 'Value'].values, alpha=0.5, label='cher')\n",
    "\n",
    "# Set the labels and title\n",
    "ax.set_xlabel('Sentiment Score')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Sentiment Scores')\n",
    "\n",
    "# Show the legend\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3fe644d",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Twitter Descriptions\n",
    "\n",
    "In this section, define two sets of emojis you designate as positive and negative. Make sure to have at least 10 emojis per set. You can learn about the most popular emojis on Twitter at [the emojitracker](https://emojitracker.com/). \n",
    "\n",
    "Associate your positive emojis with a score of +1, negative with -1. Score the average sentiment of your two artists based on the Twitter descriptions of their followers. The average sentiment can just be the total score divided by number of followers. You do not need to calculate sentiment on non-emoji content for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86485839",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emojis = [\n",
    "    \"üòÑ\", \"üòä\", \"üôÇ\", \"ü•∞\", \"ü§ó\", \"üåû\", \"üåü\", \"üíñ\", \"üéâ\", \"üéà\",\n",
    "    \"üå∫\", \"üåà\", \"üçÄ\", \"ü•≥\", \"üëè\", \"üôå\", \"üåº\", \"ü•á\", \"üåª\", \"üéµ\",\n",
    "    \"üé∂\", \"üéÅ\", \"üå∏\", \"üíÉ\", \"üî•\", \"üíØ\", \"üçì\", \"ü•Ç\", \"üåπ\", \"üíï\"\n",
    "]\n",
    "\n",
    "negative_emojis = [\n",
    "    \"üòû\", \"üòî\", \"üò¢\", \"üò†\", \"üòí\", \"üòï\", \"üòñ\", \"üò£\", \"üò©\", \"üôÅ\",\n",
    "    \"üò≠\", \"üò°\", \"üòü\", \"üò£\", \"üòì\", \"ü§î\", \"üëé\", \"ü§¨\", \"üò≥\", \"üòë\",\n",
    "    \"ü§Ø\", \"ü§¢\", \"üò¥\", \"üëø\", \"üò§\", \"üò™\", \"üò´\", \"üëø\", \"üòæ\", \"ü§¶\"\n",
    "]\n",
    "\n",
    "# Create a dataframe with emojis and sentiment values\n",
    "emoji_df = pd.DataFrame({\n",
    "    'emoji': positive_emojis + negative_emojis,\n",
    "    'sentiment': [1] * len(positive_emojis) + [-1] * len(negative_emojis)\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f2c0fc",
   "metadata": {},
   "source": [
    "Create a new dictionary of only the emojis that are present for each artist in the followers dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a5c1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_emojis = {}\n",
    "\n",
    "# Iterate over each artist in the twitter_data dictionary\n",
    "for artist, tweets in twitter_data.items():\n",
    "    emojis = []\n",
    "\n",
    "    # Iterate over each tweet of the artist\n",
    "    for tweet in tweets:\n",
    "        # Find all emojis in the tweet\n",
    "        emoji_list = [c for c in tweet if emoji.is_emoji(c)]\n",
    "\n",
    "        # Add the found emojis to the list\n",
    "        emojis.extend(emoji_list)\n",
    "\n",
    "    # Add the list of emojis to the dictionary for the artist\n",
    "    twitter_emojis[artist] = emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f3cea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new dictionary that is empty - will add cleaned data as it's processed \n",
    "twitter_data_cleaned = {}\n",
    "\n",
    "# Fold to lowercase and populate the cleaned dictionary\n",
    "for artist, tweets in twitter_data.items():\n",
    "    cleaned_tweets = {}\n",
    "    for idx, words in enumerate(tweets):\n",
    "        cleaned_words = words.lower()\n",
    "        cleaned_tweets[f'tweet{idx+1}'] = cleaned_words\n",
    "    twitter_data_cleaned[artist] = cleaned_tweets\n",
    "\n",
    "# Remove stopwords directly from twitter_data_cleaned\n",
    "for artist, tweets in twitter_data_cleaned.items():\n",
    "    for tweet, words in tweets.items():\n",
    "        # Split the words into individual tokens\n",
    "        tokens = words.split()\n",
    "        # Remove stopwords from the list of tokens\n",
    "        cleaned_tokens = [token for token in tokens if token.lower() not in sw]\n",
    "        # Join the cleaned tokens back into a single string\n",
    "        cleaned_words = \" \".join(cleaned_tokens)\n",
    "        # Update the words in the twitter_data_cleaned\n",
    "        twitter_data_cleaned[artist][tweet] = cleaned_words\n",
    "\n",
    "# Remove punctuation directly from twitter_data_cleaned\n",
    "for artist, tweets in twitter_data_cleaned.items():\n",
    "    cleaned_tweets = {}\n",
    "    for tweet, words in tweets.items():\n",
    "        cleaned_words = ''.join(character for character in words if character not in string.punctuation)\n",
    "        cleaned_tweets[tweet] = cleaned_words\n",
    "    twitter_data_cleaned[artist] = cleaned_tweets\n",
    "\n",
    "# split at whitespace \n",
    "for artist, tweets in twitter_data_cleaned.items():\n",
    "    for tweet, words in tweets.items():\n",
    "        split_words = words.split()\n",
    "        twitter_data_cleaned[artist][tweet] = split_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a9e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of outer keys\n",
    "for artist, tweets in twitter_data_cleaned.items():\n",
    "    word_list = []\n",
    "    for tweet, words in tweets.items():\n",
    "        word_list.extend(words)\n",
    "    twitter_data_cleaned[artist] = word_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "362d9c6e",
   "metadata": {},
   "source": [
    "Creating list/ dict of emojis from each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb7a1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists \n",
    "cher_tweets = twitter_data_cleaned['cher'] \n",
    "\n",
    "cher_emoji_counter = {}\n",
    "\n",
    "for item in cher_tweets:\n",
    "    emoji_list = emojis.get(item)\n",
    "    for emoji in emoji_list:\n",
    "        if emoji in cher_emoji_counter:\n",
    "            cher_emoji_counter[emoji] += 1\n",
    "        else:\n",
    "            cher_emoji_counter[emoji] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "384ee862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists \n",
    "robyn_tweets = twitter_data_cleaned['robynkonichiwa']\n",
    "\n",
    "robyn_emoji_counter = {}\n",
    "\n",
    "for item in robyn_tweets:\n",
    "    emoji_list = emojis.get(item)\n",
    "    for emoji in emoji_list:\n",
    "        if emoji in robyn_emoji_counter:\n",
    "            robyn_emoji_counter[emoji] += 1\n",
    "        else:\n",
    "            robyn_emoji_counter[emoji] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3cc03c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentiment for Cher:  0.8833180020673501\n",
      "Average Sentiment for Robyn:  0.9071688140072679\n"
     ]
    }
   ],
   "source": [
    "## Cher \n",
    "total_sentiment_sum = 0\n",
    "emoji_count = 0\n",
    "\n",
    "for emoji, count in cher_emoji_counter.items():\n",
    "    if emoji in emoji_df['emoji'].values:\n",
    "        sentiment_value = emoji_df.loc[emoji_df['emoji'] == emoji, 'sentiment'].values[0]\n",
    "        total_sentiment_sum += sentiment_value * count\n",
    "        emoji_count += count\n",
    "\n",
    "average_sentiment = total_sentiment_sum / emoji_count if emoji_count != 0 else 0\n",
    "\n",
    "print(\"Average Sentiment for Cher: \", average_sentiment)\n",
    "\n",
    "## Robyn\n",
    "total_sentiment_sum = 0\n",
    "emoji_count = 0\n",
    "\n",
    "for emoji, count in robyn_emoji_counter.items():\n",
    "    if emoji in emoji_df['emoji'].values:\n",
    "        sentiment_value = emoji_df.loc[emoji_df['emoji'] == emoji, 'sentiment'].values[0]\n",
    "        total_sentiment_sum += sentiment_value * count\n",
    "        emoji_count += count\n",
    "\n",
    "average_sentiment = total_sentiment_sum / emoji_count if emoji_count != 0 else 0\n",
    "\n",
    "print(\"Average Sentiment for Robyn: \", average_sentiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb92eb93",
   "metadata": {},
   "source": [
    "Q: What is the average sentiment of your two artists? \n",
    "\n",
    "A: \n",
    "- Average Sentiment for Cher:  0.8833180020673501\n",
    "- Average Sentiment for Robyn:  0.9071688140072679\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cae3ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_filtered_emojis = {emoji: count for emoji, count in cher_emoji_counter.items() if emoji in emoji_df['emoji'].values}\n",
    "\n",
    "robyn_filtered_emojis = {emoji: count for emoji, count in robyn_emoji_counter.items() if emoji in emoji_df['emoji'].values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c44849c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find most popular positive and negative by associating the counts with the values and sorting\n",
    "## Cher\n",
    "cher_filtered_emojis_df= pd.DataFrame(list(cher_filtered_emojis.items()), columns=['emoji', 'count'])\n",
    "cher_emoji_values = pd.merge(cher_filtered_emojis_df, emoji_df, on = 'emoji')\n",
    "cher_emoji_values['value'] = cher_emoji_values['count'] * cher_emoji_values['sentiment']\n",
    "\n",
    "cher_negative_emoji = cher_emoji_values.sort_values(by = 'value').head(1)['emoji'].values\n",
    "cher_positive_emoji = cher_emoji_values.sort_values(by = 'value', ascending= False).head(1)['emoji'].values\n",
    "\n",
    "## Robyn \n",
    "robyn_filtered_emojis_df= pd.DataFrame(list(robyn_filtered_emojis.items()), columns=['emoji', 'count'])\n",
    "robyn_emoji_values = pd.merge(robyn_filtered_emojis_df, emoji_df, on = 'emoji')\n",
    "robyn_emoji_values['value'] = robyn_emoji_values['count'] * robyn_emoji_values['sentiment']\n",
    "\n",
    "robyn_negative_emoji = robyn_emoji_values.sort_values(by = 'value').head(1)['emoji'].values\n",
    "robyn_positive_emoji = robyn_emoji_values.sort_values(by = 'value', ascending= False).head(1)['emoji'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e803c4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chers most popular positive emoji: ['üåà']\n",
      "Chers most popular negative emoji: ['ü§î']\n",
      "Robyns most popular positive emoji: ['üåà']\n",
      "Robyns most popular negative emoji: ['ü§î']\n"
     ]
    }
   ],
   "source": [
    "print('Chers most popular positive emoji:', cher_positive_emoji)\n",
    "print('Chers most popular negative emoji:', cher_negative_emoji)\n",
    "\n",
    "print('Robyns most popular positive emoji:', robyn_positive_emoji)\n",
    "print('Robyns most popular negative emoji:', robyn_negative_emoji)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a81ac8c8",
   "metadata": {},
   "source": [
    "\n",
    "Q: Which positive emoji is the most popular for each artist? Which negative emoji? \n",
    "\n",
    "A: \n",
    "- Chers most popular positive emoji: ['üåà']\n",
    "- Chers most popular negative emoji: ['ü§î']\n",
    "- Robyns most popular positive emoji: ['üåà']\n",
    "- Robyns most popular negative emoji: ['ü§î']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
